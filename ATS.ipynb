{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5vvJhQVZuWb",
        "outputId": "13979ae5-3e1b-47af-eaca-59b5c2ad53a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: docx2txt in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.8)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install docx2txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'PyPDF2'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstreamlit\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mst\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mPyPDF2\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdocx\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'PyPDF2'"
          ]
        }
      ],
      "source": [
        "import streamlit as st\n",
        "import PyPDF2\n",
        "import docx\n",
        "import nltk\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from io import StringIO\n",
        "import string\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "cv = CountVectorizer()\n",
        "\n",
        "\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(file):\n",
        "    pdf_reader = PyPDF2.PdfReader(file)\n",
        "    text = ''\n",
        "    for page_num in range(len(pdf_reader.pages)):\n",
        "        page = pdf_reader.pages[page_num]\n",
        "        text += page.extract_text()\n",
        "    return text\n",
        "# Function to extract text from DOCX\n",
        "def extract_text_from_docx(file):\n",
        "    doc = docx.Document(file)\n",
        "    text = ''\n",
        "    for para in doc.paragraphs:\n",
        "        text += para.text\n",
        "    return text\n",
        "# Function to preprocess text\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove leading/trailing whitespace\n",
        "    text = text.strip()\n",
        "    \n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    \n",
        "    # Remove any extra spaces between words\n",
        "    text = ' '.join(text.split())\n",
        "    \n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def removing_stopwords(texts):\n",
        "    sw= set(stopwords.words(\"english\"))\n",
        "    tokenize_word_resume = word_tokenize(texts)\n",
        "    text = []\n",
        "    for word in tokenize_word_resume:\n",
        "        if word not in sw :\n",
        "            text.append(word)\n",
        "            text = \" \".join(texts)\n",
        "            return text\n",
        "\n",
        "\n",
        "# Function to calculate similarity using TF-IDF and cosine similarity\n",
        "def calculate_similarity(resume_text, job_description_text):\n",
        "    # Preprocess the text\n",
        "    resume_texts = preprocess_text(resume_text)\n",
        "    job_description_texts = preprocess_text(job_description_text)\n",
        "    \n",
        "    \n",
        "    resume_stopwords =removing_stopwords(resume_texts)\n",
        "    desc_stopwords =removing_stopwords(job_description_texts)\n",
        "\n",
        "\n",
        "    content = [resume_stopwords,desc_stopwords]\n",
        "    matrix = cv.fit_transform(content)\n",
        "    \n",
        "    # Compute cosine similarity\n",
        "    similarity_mat = cosine_similarity(matrix)\n",
        "    print(\"Resume matches by :\" + str(similarity_mat[1][0]* 100)+ \"%\")\n",
        "    return similarity_mat\n",
        "# Streamlit UI\n",
        "st.title(\"ATS Resume Checker with Machine Learning\")\n",
        "st.write(\"Upload your resume and the job description to check the ATS score using a machine learning model.\")\n",
        "# Upload resume file\n",
        "resume_file = st.file_uploader(\"Upload your resume\", type=[\"pdf\", \"docx\", \"txt\"])\n",
        "job_description = st.text_area(\"Paste the job description\")\n",
        "if resume_file is not None and job_description:\n",
        "    # Extract text from the uploaded resume file\n",
        "    if resume_file.type == \"application/pdf\":\n",
        "        resume_text = extract_text_from_pdf(resume_file)\n",
        "    elif resume_file.type == \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\":\n",
        "        resume_text = extract_text_from_docx(resume_file)\n",
        "    else:\n",
        "        stringio = StringIO(resume_file.getvalue().decode(\"utf-8\"))\n",
        "        resume_text = stringio.read()\n",
        "    # Calculate similarity score using the model\n",
        "    similarity_mat = calculate_similarity(resume_text, job_description)\n",
        "\n",
        "\n",
        "    \n",
        "    st.write(f\"**ATS Score: {str(similarity_mat[1][0]* 100)}%**\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'nltk' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt_tab\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipiw38xmZ-6b",
        "outputId": "aa851928-8eb1-4a94-af5d-f8b51ff594ad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import docx2txt\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VxtS1v9KHOuV",
        "outputId": "12557157-d915-4ece-cc89-156dbf40af05"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "string.punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oT_ptAXPVQK5",
        "outputId": "90561f9f-2527-465e-fd04-c388456209a5"
      },
      "outputs": [],
      "source": [
        "\n",
        "descrption='''Currently pursuing or recently completed a degree in Data Science, Statistics, Computer Science, Mathematics, or a related field.\n",
        "2. Strong programming skills in Python or R.\n",
        "3. Familiarity with data manipulation libraries such as Pandas and NumPy.\n",
        "4. Basic understanding of machine learning algorithms (e.g., regression, classification, clustering).\n",
        "5. Experience with data visualization tools like Matplotlib, Seaborn, or other relevant libraries.\n",
        "6. Knowledge of SQL for data extraction and querying.\n",
        "7. Strong analytical thinking and problem-solving skills.\n",
        "8. Ability to work independently and as part of a collaborative team.\n",
        "9. Excellent communication skills to present findings and insights.\n",
        "Skill(s) required\n",
        "Data Analytics\n",
        "Data Science\n",
        "Deep Learning\n",
        "Machine Learning\n",
        "Natural Language Processing (NLP)\n",
        "Python\n",
        "SQL'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "wh3EX3kwJnQq"
      },
      "outputs": [],
      "source": [
        "#Cleaning job description\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bh20wROvHuaR",
        "outputId": "bf8b9085-e0e7-47fe-fcf5-e9c0452e58ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Currently pursuing or recently completed a degree in Data Science Statistics Computer Science Mathematics or a related field\n",
            "2 Strong programming skills in Python or R\n",
            "3 Familiarity with data manipulation libraries such as Pandas and NumPy\n",
            "4 Basic understanding of machine learning algorithms eg regression classification clustering\n",
            "5 Experience with data visualization tools like Matplotlib Seaborn or other relevant libraries\n",
            "6 Knowledge of SQL for data extraction and querying\n",
            "7 Strong analytical thinking and problemsolving skills\n",
            "8 Ability to work independently and as part of a collaborative team\n",
            "9 Excellent communication skills to present findings and insights\n",
            "Skills required\n",
            "Data Analytics\n",
            "Data Science\n",
            "Deep Learning\n",
            "Machine Learning\n",
            "Natural Language Processing NLP\n",
            "Python\n",
            "SQL\n"
          ]
        }
      ],
      "source": [
        "jobs_desc_clean = \"\".join([i for i in descrption if i not in string.punctuation])\n",
        "print(jobs_desc_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8kysLaLUaIo4",
        "outputId": "71904f3d-0792-48ab-b4ff-cfdad9e798e6"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Q6EIHBqoW5Op"
      },
      "outputs": [],
      "source": [
        "resume_text = docx2txt.process(r\"C:\\Users\\ASUS\\OneDrive\\Documents/My resume.docx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkR50_9vXx7X",
        "outputId": "5e0d8573-3222-47e1-af5f-07763f315afd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIqe7Kt4aK2Z",
        "outputId": "b74074a2-5899-4d01-df14-760f315746ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Udit Sharma\n",
            "\n",
            "Data Scientist\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Contact\n",
            "\n",
            "s36 sidheshwar nagar\n",
            "\n",
            "sipri bazarjhansi\n",
            "\n",
            "\n",
            "\n",
            "emailudits4513gmailcom\n",
            "\n",
            "\n",
            "\n",
            "Objective\n",
            "\n",
            "Data science professional with handson experience in Data Analytics Statistical Modelling Machine Learning Python and SQL Targeting assignments in Data ScienceData Analysis with reputed organization Eager to contribute to a dynamic team by solving complex problems and driving informed decisionmaking through data\n",
            "\n",
            "Education\n",
            "\n",
            "Bachelor of Science MathematicsBSc 062024\n",
            "\n",
            "Bundelkhand University – JhansiUP\n",
            "\n",
            "\n",
            "\n",
            "Key Skills\n",
            "\n",
            "NumPy\n",
            "\n",
            "Pandas\n",
            "\n",
            "Django\n",
            "\n",
            "Matplotlib\n",
            "\n",
            "Scikitlearn\n",
            "\n",
            "Python\n",
            "\n",
            "SQL\n",
            "\n",
            "Data Mining\n",
            "\n",
            "Data Preprocessing\n",
            "\n",
            "Machine learning\n",
            "\n",
            "Power BI\n",
            "\n",
            "ChatGPT\n",
            "\n",
            "Predictive Modelling\n",
            "\n",
            "Statistical Analysis\n",
            "\n",
            "Data visualization\n",
            "\n",
            "Disciplined and Organized\n",
            "\n",
            "Time Management\n",
            "\n",
            "Adaptive and Punctual\n",
            "\n",
            "Communication Skills\n",
            "\n",
            "WEBSITES PORTFOLIOS AND PROJECTS\n",
            "\n",
            "httpswwwlinkedincominrahulshrivastavabab81740 \n",
            "\n",
            "udit16561 udit sharma githubcom\n",
            "\n",
            "\n",
            "\n",
            "CERTIFICATION\n",
            "\n",
            "Certified Data Scientist\n",
            "\n",
            "Certified Power BI Developer\n",
            "\n",
            "Certified UIUX Developer\n",
            "\n",
            "‘O’ level  Certified\n"
          ]
        }
      ],
      "source": [
        "# Cleaning Resume Text\n",
        "resume_clean= \"\".join([j for j in resume_text if j not in string.punctuation])\n",
        "print(resume_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7FH_qtzXaK7c"
      },
      "outputs": [],
      "source": [
        "sw= set(stopwords.words(\"english\"))\n",
        "tokenize_word_resume = word_tokenize(resume_clean)\n",
        "without_stopword_resume = []\n",
        "for word in tokenize_word_resume:\n",
        "  if word not in sw :\n",
        "    without_stopword_resume.append(word)\n",
        "clean_res = \" \".join(without_stopword_resume)\n",
        "\n",
        "tokenize_word_jobs = word_tokenize(jobs_desc_clean)\n",
        "without_stopword_jobdecr = []\n",
        "for k in tokenize_word_jobs:\n",
        "  if k not in sw :\n",
        "    without_stopword_jobdecr.append(k)\n",
        "\n",
        "clean_dcsr = \" \".join( without_stopword_jobdecr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nOzqIwoZOD1E"
      },
      "outputs": [],
      "source": [
        "content = [clean_dcsr,clean_res]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3yt3aKGUbcp_"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "oEgTn6V1btL1"
      },
      "outputs": [],
      "source": [
        "matrix = cv.fit_transform(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kMM57Wekb4cN"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "def similarity_ats(file):\n",
        "    content = [clean_dcsr,file]\n",
        "    from sklearn.feature_extraction.text import CountVectorizer\n",
        "    cv = CountVectorizer()\n",
        "    matrix = cv.fit_transform(content)\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    similarity_mat = cosine_similarity(matrix)\n",
        "    print(\"Resume matches by :\" + str(similarity_mat[1][0]* 100)+ \"%\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "a0rdxmsucHYl"
      },
      "outputs": [],
      "source": [
        "similarity_mat = cosine_similarity(matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmT9G6pHcyXu",
        "outputId": "3979b768-3368-4a43-8d64-81f81bb2c9c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.         0.53117518]\n",
            " [0.53117518 1.        ]]\n"
          ]
        }
      ],
      "source": [
        "print(similarity_mat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObSxwzvDc7EV",
        "outputId": "61b40590-6438-47e2-b3d5-0e8e61f42b4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resume matches by :0.5311751845367257\n"
          ]
        }
      ],
      "source": [
        "print(\"Resume matches by :\" + str(similarity_mat[0][1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcvje21GdTID",
        "outputId": "26f8daab-34ec-42b3-c3a3-19b666ce6286"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resume matches by :53.117518453672574%\n"
          ]
        }
      ],
      "source": [
        "print(\"Resume matches by :\" + str(similarity_mat[1][0]* 100)+ \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "#pickle.dump(dt,open('movie_dict.pkl','wb'))\n",
        "pickle.dump(similarity_ats,open('similarity_mat.pkl','wb'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
